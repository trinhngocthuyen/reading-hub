{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Probability\n",
    "https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573\n",
    "\n",
    "## Chap 4. Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms/Highlights\n",
    "\n",
    "4 new distributions were introduced: \n",
    "- Geometric distribution\n",
    "- First success distribution\n",
    "- Negative binomial distribution\n",
    "- Poisson distribution\n",
    "\n",
    "The form of Poisson PMF is a bit like Taylor series. Think of it in your proof (if analytical sums appear).\n",
    "\n",
    "This chapter introduced 3 ways to compute the expectation of r.v.s\n",
    "- By **LOTUS**: sometimes require complicated analytical transformation.\n",
    "- Via **indicator r.v.s**: This technique is based on\n",
    "  - The *fundamental bridge* (betweek probability and expectation)\n",
    "  - The *linearity of expectation*.\n",
    "- Via **survival function** (think of CDF)\n",
    "\n",
    "Connections between Poisson, Binomial and Hypergeometric distributions\n",
    "- Taking limits: $HGeom \\to Bin \\to Pois$\n",
    "- Conditioning: $Pois \\to Bin \\to HGeom$\n",
    "\n",
    "P/s: What impressed me the most is the intermediate representation of a distribution. It helps break the problem into small pieces like *divide & conquer*. Particularly, despite looking trivial, indicator r.v.s bring the fundamental bridge as a power to tackle problems with less effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More details\n",
    "- **Geometric distribution**\n",
    "  - Denotes the number of failures before a success.\n",
    "  - $X \\sim Geom(p)$\n",
    "  - *PMF*: $P(X=k) = q^k p$\n",
    "- **First success distribution**\n",
    "  - Like geometric distribution, but include the success.\n",
    "  - $X \\sim FS(p)$. From the story $\\implies X-1 \\sim Geom(p)$\n",
    "- **Negative Binomial distribution**\n",
    "  - Denotes the number of failures before the $r^th$ success.\n",
    "  - $X \\sim NBin(r, p)$\n",
    "  - *PMF*: $P(X=k) = {n+r-1 \\choose r-1} p^r q^n$\n",
    "\n",
    "|   | With replacement | Without replacement |\n",
    "| --- | --- | --- |\n",
    "| **Fixed number of trials** (n) | Binomial | Hypergeometric |\n",
    "| **Fixed number of successes** (r) | Negative binomial | Negative hypergeometric |\n",
    "\n",
    "- **Expectation**: Interchangeable terms: *expected value*, *expectation*, *mean*\n",
    "  - $E(X) = \\sum_{j=1}^{\\infty} x_j P(X = x_j)$\n",
    "- **Linearity** of expectation:\n",
    "  - $E(X+Y) = E(X) + E(Y)$\n",
    "  - Note: independence is not needed\n",
    "\n",
    "- $NBin \\leftrightarrow Geom$:\n",
    "  - $X \\sim NBin(n,p)$. Then $X = X_1 + ... + X_r$ where $X_j$ are i.i.d. $Geom(p)$.\n",
    "\n",
    "- **Indicator r.v.s** are pretty helpful since they provide the **fundamental bridge** (a link between probability and expectation):\n",
    "  - $P(A) = E(I_A)$\n",
    "- Expectation via **survival function**\n",
    "  - Survival function: $G(x) = 1-F(x) = P(X > x)$\n",
    "  - $E(X) = \\sum_{n=0}^{\\infty} G(n)$\n",
    "- **LOTUS** (Law of the unconscious statiscian)\n",
    "  - $E(g(X)) = \\sum_x g(x) P(X=x)$\n",
    "  - \"Unconscious\" in the name refers to the replacement from $x$ to $g(x)$ in the definition :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Poisson distribution**\n",
    "  - Denotes the number of successes in a particular region/interval of time, and there are a large number of trials, each of which has a small probability of success.\n",
    "  - $X \\sim Pois(\\lambda)$\n",
    "  - *PMF*: $P(X=k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}$\n",
    "  - $\\lambda$ is interpreted as the *rate of occurrence*.\n",
    "- **Poisson paradigm** (or the *law of rare events*)\n",
    "  - The number of events $A_1, ..., A_n$ occurring with probabilities $p_1, ..., p_n$ could be approximated by $Pois(\\lambda)$, where $\\lambda = \\sum_{j=1}^n p_j$. Conditions for this approximation:\n",
    "    - $n$ is large, the $p_j$ are small.\n",
    "    - The $A_j$ are independent or weakly dependent. (How weak? There are some measurements)\n",
    "- Sum of independent Possions:\n",
    "  - $X$ is indep. of $Y$, and $X \\sim Pois(\\lambda_1), Y \\sim Pois(\\lambda_2) \\implies X+Y \\sim Pois(\\lambda_1+\\lambda_2)$\n",
    "\n",
    "Connections between Poissons and Binomials:\n",
    "- Poisson approximation to Binomial:\n",
    "  - If $X \\sim Bin(n,p)$ and $n \\to \\infty, p \\to 0$ s.t. $np$ converges to a constant $\\lambda$, then the PMF of $X$ converges to $Pois(\\lambda)$ PMF.\n",
    "- Poisson given a sum of Poissons:\n",
    "  - $X$ is indep. of $Y$. If $X \\sim Pois(\\lambda_1), Y \\sim Pois(\\lambda_2)$, then the conditional distribution of $X$ given $X+Y=n$ is $Bin(n, \\lambda_1/(\\lambda_1+\\lambda_2))$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Variance**\n",
    "  - Measures how spread out the data is.\n",
    "  - $Var(X) = E(X-EX)^2 = E(X^2) - (EX)^2$\n",
    "  - Note that variance is not linear, except the case of independence.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
